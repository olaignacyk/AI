{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pygame\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from enum import Enum\n",
    "\n",
    "class Actions(Enum):\n",
    "    RIGHT = 0\n",
    "    UP = 1\n",
    "    LEFT = 2\n",
    "    DOWN = 3\n",
    "\n",
    "class CrossTheRoadEnv(gym.Env):\n",
    "    metadata = {\"render_modes\": [\"human\"], \"render_fps\": 4}\n",
    "\n",
    "    def __init__(self, render_mode=None, size=(7, 10)):\n",
    "        self.grid_width, self.grid_height = size\n",
    "        self.render_mode = render_mode\n",
    "        self.window_size = 512\n",
    "\n",
    "        self.observation_space = spaces.Box(low=0, high=max(size), shape=(4,), dtype=np.float32)\n",
    "        self.action_space = spaces.Discrete(4)\n",
    "\n",
    "        self._action_to_direction = {\n",
    "            Actions.RIGHT.value: np.array([1, 0]),\n",
    "            Actions.UP.value: np.array([0, 1]),\n",
    "            Actions.LEFT.value: np.array([-1, 0]),\n",
    "            Actions.DOWN.value: np.array([0, -1]),\n",
    "        }\n",
    "\n",
    "        self.lanes = [3, 4, 6, 7]\n",
    "        self.agent_pos = None\n",
    "        self.goal_pos = None\n",
    "        self.cars = []\n",
    "        self.window = None\n",
    "        self.clock = None\n",
    "        self.car_step_counter = 0\n",
    "        self.car_step_frequency = 1 \n",
    "        self.agent_image = None\n",
    "        self.goal_image = None\n",
    "        self.car_image = None\n",
    "\n",
    "\n",
    "    def _generate_cars(self):\n",
    "        self.cars = []\n",
    "\n",
    "        # Na ka≈ºdym pasie 2 samochody w tych samych kolumnach\n",
    "        for lane in self.lanes:\n",
    "            direction = 1 if lane % 2 == 0 else -1\n",
    "            positions = [0, 3]  # ustalone startowe kolumny\n",
    "            for x in positions:\n",
    "                self.cars.append({\n",
    "                    \"start_x\": x,\n",
    "                    \"pos\": np.array([x, lane]),\n",
    "                    \"dir\": direction\n",
    "                })\n",
    "\n",
    "\n",
    "    def _move_cars(self):\n",
    "        for car in self.cars:\n",
    "            car[\"pos\"][0] = (car[\"pos\"][0] + car[\"dir\"]) % self.grid_width\n",
    "\n",
    "    def _get_obs(self):\n",
    "        return np.concatenate((self.agent_pos, self.goal_pos)).astype(np.float32)\n",
    "\n",
    "\n",
    "    def _is_collision(self):\n",
    "        return any(np.array_equal(self.agent_pos, car[\"pos\"]) for car in self.cars)\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        self.agent_pos = np.array([self.grid_width // 2, 0])\n",
    "        self.goal_pos = np.array([self.grid_width // 2, self.grid_height - 1])\n",
    "        self._generate_cars()\n",
    "        return self._get_obs(), {}\n",
    "\n",
    "    def step(self, action):\n",
    "        direction = self._action_to_direction[int(action)]\n",
    "        old_x, old_y = self.agent_pos\n",
    "\n",
    "        self.agent_pos = np.clip(self.agent_pos + direction, [0, 0], [self.grid_width - 1, self.grid_height - 1])\n",
    "        self._move_cars()\n",
    "\n",
    "        reward = 0.0\n",
    "        terminated = False\n",
    "\n",
    "        if np.array_equal(self.agent_pos, self.goal_pos):\n",
    "            reward = 4.0\n",
    "            terminated = True  # epizod siƒô ko≈Ñczy\n",
    "        elif self._is_collision():\n",
    "            reward = -2.0\n",
    "            # terminated = True\n",
    "            self.agent_pos = np.array([self.grid_width / 2, 0])  # reset agenta\n",
    "        elif self.agent_pos[1] > old_y:\n",
    "            reward = 0.2\n",
    "        elif self.agent_pos[1] < old_y:\n",
    "            reward = -0.1\n",
    "        elif self.agent_pos[1] == old_y and self.agent_pos[0] == old_x:\n",
    "            reward = -0.1\n",
    "        else:\n",
    "            reward = -0.1\n",
    "\n",
    "\n",
    "        return self._get_obs(), reward, terminated, False, {}\n",
    "\n",
    " \n",
    "\n",
    "    def render(self):\n",
    "        if self.render_mode != \"human\":\n",
    "            return\n",
    "\n",
    "        if self.window is None:\n",
    "            pygame.init()\n",
    "            self.window = pygame.display.set_mode((self.window_size, self.window_size))\n",
    "\n",
    "        if self.clock is None:\n",
    "            self.clock = pygame.time.Clock()\n",
    "\n",
    "        cell_size = self.window_size // self.grid_height\n",
    "\n",
    "        # Za≈Çaduj obrazy tylko raz\n",
    "        if self.agent_image is None:\n",
    "            self.agent_image = pygame.image.load(\"images/zaba.png\")\n",
    "            self.agent_image = pygame.transform.scale(self.agent_image, (cell_size, cell_size))\n",
    "\n",
    "        if self.goal_image is None:\n",
    "            self.goal_image = pygame.image.load(\"images/goal.png\")\n",
    "            self.goal_image = pygame.transform.scale(self.goal_image, (cell_size, cell_size))\n",
    "\n",
    "        if self.car_image is None:\n",
    "            self.car_image = pygame.image.load(\"images/car.png\")\n",
    "            self.car_image = pygame.transform.scale(self.car_image, (cell_size, cell_size))\n",
    "\n",
    "        self.window.fill((255, 255, 255))  # t≈Ço\n",
    "\n",
    "        # Rysowanie agenta\n",
    "        agent_pos_px = (self.agent_pos[0]*cell_size,\n",
    "                        self.window_size - (self.agent_pos[1]+1)*cell_size)\n",
    "        self.window.blit(self.agent_image, agent_pos_px)\n",
    "\n",
    "        # Rysowanie celu\n",
    "        goal_pos_px = (self.goal_pos[0]*cell_size,\n",
    "                       self.window_size - (self.goal_pos[1]+1)*cell_size)\n",
    "        self.window.blit(self.goal_image, goal_pos_px)\n",
    "\n",
    "        # Rysowanie samochod√≥w\n",
    "        for car in self.cars:\n",
    "            car_pos_px = (car[\"pos\"][0]*cell_size,\n",
    "                          self.window_size - (car[\"pos\"][1]+1)*cell_size)\n",
    "            self.window.blit(self.car_image, car_pos_px)\n",
    "\n",
    "        pygame.display.flip()\n",
    "        self.clock.tick(self.metadata[\"render_fps\"])\n",
    "\n",
    "    def close(self):\n",
    "        if self.window:\n",
    "            pygame.quit()\n",
    "            self.window = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class PolicyNet(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64), nn.ReLU(),\n",
    "            nn.Linear(64, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.softmax(self.net(x), dim=-1)\n",
    "\n",
    "# REINFORCE agent\n",
    "class PGAgent:\n",
    "    def __init__(self, obs_dim, act_dim, lr=1e-3):\n",
    "        self.policy = PolicyNet(obs_dim, act_dim)\n",
    "        self.optimizer = optim.Adam(self.policy.parameters(), lr=lr)\n",
    "        self.gamma = 0.99\n",
    "\n",
    "        self.log_probs = []\n",
    "        self.rewards = []\n",
    "\n",
    "    def select_action(self, state, explore=True):\n",
    "        state = torch.FloatTensor(state)\n",
    "        probs = self.policy(state)\n",
    "        dist = torch.distributions.Categorical(probs)\n",
    "        if explore:\n",
    "            action = dist.sample()\n",
    "            self.log_probs.append(dist.log_prob(action))\n",
    "        else:\n",
    "            action = torch.argmax(probs)  # deterministyczna akcja\n",
    "        return action.item()\n",
    "\n",
    "\n",
    "    def update(self):\n",
    "        R = 0\n",
    "        returns = []\n",
    "        for r in reversed(self.rewards):\n",
    "            R = r + self.gamma * R\n",
    "            returns.insert(0, R)\n",
    "\n",
    "        returns = torch.tensor(returns)\n",
    "        returns = (returns - returns.mean()) / (returns.std() + 1e-8)\n",
    "\n",
    "        # loss = -torch.stack(self.log_probs) * returns\n",
    "        # loss = loss.sum()\n",
    "        log_probs = torch.stack(self.log_probs)\n",
    "        entropy = -log_probs.mean()  # entropia ~ eksploracja\n",
    "        loss = -(log_probs * returns).sum() - 0.01 * entropy\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        self.log_probs = []\n",
    "        self.rewards = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN] Epizod 0, suma nagr√≥d: -17.40\n"
     ]
    }
   ],
   "source": [
    "# env = CrossTheRoadEnv(render_mode=\"human\")\n",
    "# agent = PGAgent(obs_dim=4, act_dim=4)\n",
    "\n",
    "# EPISODES = 3000\n",
    "# RENDER_EVERY = 200\n",
    "\n",
    "# for episode in range(EPISODES):\n",
    "#     state, _ = env.reset()\n",
    "#     done = False\n",
    "#     ep_reward = 0\n",
    "\n",
    "#     while not done:\n",
    "#         action = agent.select_action(state)\n",
    "#         next_state, reward, done, truncated, _ = env.step(action)\n",
    "#         agent.rewards.append(reward)\n",
    "#         ep_reward += reward\n",
    "#         state = next_state\n",
    "\n",
    "#         if episode % RENDER_EVERY == 0:\n",
    "#             env.render()\n",
    "\n",
    "#     agent.update()\n",
    "\n",
    "#     if episode % 100 == 0:\n",
    "#         print(f\"Ep {episode}, reward: {ep_reward:.2f}\")\n",
    "\n",
    "# env.close()\n",
    "# env = CrossTheRoadEnv(render_mode=\"human\")\n",
    "# agent = PGAgent(obs_dim=4, act_dim=4)\n",
    "\n",
    "\n",
    "# EPISODES = 3000\n",
    "# RENDER_EVERY = 200\n",
    "# MAX_STEPS = 200\n",
    "\n",
    "# for episode in range(EPISODES):\n",
    "#     state, _ = env.reset()\n",
    "#     done = False\n",
    "#     ep_reward = 0\n",
    "#     step_count = 0\n",
    "\n",
    "#     while not done and step_count < MAX_STEPS:\n",
    "#         action = agent.select_action(state)\n",
    "#         next_state, reward, done, truncated, _ = env.step(action)\n",
    "#         agent.rewards.append(reward)\n",
    "#         ep_reward += reward\n",
    "#         state = next_state\n",
    "#         step_count += 1\n",
    "\n",
    "#         if episode % RENDER_EVERY == 0:\n",
    "#             env.render()\n",
    "\n",
    "#     agent.update()\n",
    "\n",
    "#     if episode % 100 == 0:\n",
    "#         print(f\"Ep {episode}, reward: {ep_reward:.2f}\")\n",
    "\n",
    "# torch.save(agent.policy.state_dict(), \"policy.pt\")\n",
    "# print(\"Model zapisany jako policy.pt\")\n",
    "\n",
    "# env.close()\n",
    "\n",
    "env = CrossTheRoadEnv(render_mode=\"human\")\n",
    "agent = PGAgent(obs_dim=env.observation_space.shape[0], act_dim=env.action_space.n)\n",
    "\n",
    "EPISODES = 3000\n",
    "RENDER_EVERY = 200\n",
    "MAX_STEPS = 200\n",
    "\n",
    "# === TRENING AGENDA ===\n",
    "for episode in range(EPISODES):\n",
    "    state, _ = env.reset()\n",
    "    done = False\n",
    "    ep_reward = 0\n",
    "    step_count = 0\n",
    "\n",
    "    while not done and step_count < MAX_STEPS:\n",
    "        action = agent.select_action(state)  # explore = True domy≈õlnie\n",
    "        next_state, reward, done, truncated, _ = env.step(action)\n",
    "        agent.rewards.append(reward)\n",
    "        ep_reward += reward\n",
    "        state = next_state\n",
    "        step_count += 1\n",
    "\n",
    "        if episode % RENDER_EVERY == 0:\n",
    "            env.render()\n",
    "\n",
    "    agent.update()\n",
    "\n",
    "    if episode % 100 == 0:\n",
    "        print(f\"[TRAIN] Epizod {episode}, suma nagr√≥d: {ep_reward:.2f}\")\n",
    "\n",
    "# === ZAPIS MODELU ===\n",
    "torch.save(agent.policy.state_dict(), \"policy.pt\")\n",
    "print(\"‚úÖ Model zapisany jako policy.pt\")\n",
    "\n",
    "# === TESTOWANIE WYUCZONEGO AGENDA ===\n",
    "agent.policy.eval()  # tryb ewaluacyjny\n",
    "TEST_EPISODES = 5\n",
    "\n",
    "print(\"\\n=== TESTOWANIE WYUCZONEGO AGENDA ===\")\n",
    "for i in range(TEST_EPISODES):\n",
    "    state, _ = env.reset()\n",
    "    done = False\n",
    "    ep_reward = 0\n",
    "    step_count = 0\n",
    "\n",
    "    while not done and step_count < MAX_STEPS:\n",
    "        action = agent.select_action(state, explore=False)  # deterministycznie\n",
    "        next_state, reward, done, truncated, _ = env.step(action)\n",
    "        state = next_state\n",
    "        ep_reward += reward\n",
    "        step_count += 1\n",
    "        env.render()\n",
    "\n",
    "    print(f\"[TEST] Epizod {i+1}: suma nagr√≥d = {ep_reward:.2f}\")\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üéÆ Test wyuczonego agenta:\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mJƒÖdro Kernel uleg≈Ço awarii podczas wykonywania kodu w bie≈ºƒÖcej kom√≥rce lub w poprzedniej kom√≥rce. \n",
      "\u001b[1;31mPrzejrzyj kod w kom√≥rkach, aby zidentyfikowaƒá mo≈ºliwƒÖ przyczynƒô awarii. \n",
      "\u001b[1;31mKliknij <a href='https://aka.ms/vscodeJupyterKernelCrash'>tutaj</a>, aby uzyskaƒá wiƒôcej informacji. \n",
      "\u001b[1;31mAby uzyskaƒá dalsze szczeg√≥≈Çy, wy≈õwietl <a href='command:jupyter.viewOutput'>dziennik</a> Jupyter."
     ]
    }
   ],
   "source": [
    "# ======== TEST WYUCZONEGO AGENDA W PYGAME ========\n",
    "import time\n",
    "\n",
    "print(\"\\nüéÆ Test wyuczonego agenta:\")\n",
    "\n",
    "state, _ = env.reset()\n",
    "done = False\n",
    "test_reward = 0\n",
    "\n",
    "while not done:\n",
    "    action = agent.select_action(state)\n",
    "    state, reward, done, _, _ = env.step(action)\n",
    "    test_reward += reward\n",
    "    env.render()\n",
    "    # time.sleep(0.1)  # spowolnienie, by lepiej widzieƒá\n",
    "\n",
    "env.close()\n",
    "print(f\"‚úÖ Test zako≈Ñczony. ≈ÅƒÖczna nagroda: {test_reward:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
